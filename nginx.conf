upstream vllm_backend {
    # Use least connections load balancing - better for LLM requests
    # which can vary significantly in processing time
    least_conn;
    
    # This will be dynamically populated based on number of GPUs
    # Placeholder - will be replaced by start.sh
    server 127.0.0.1:8001 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name localhost;

    # Health check endpoint
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }

    # Proxy all requests to vLLM backend
    location / {
        proxy_pass http://vllm_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Handle large request bodies (for long prompts)
        client_max_body_size 10M;
        
        # Increase timeouts for long-running requests
        proxy_connect_timeout 60s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
        
        # Enable streaming responses
        proxy_buffering off;
        proxy_cache off;
    }
}